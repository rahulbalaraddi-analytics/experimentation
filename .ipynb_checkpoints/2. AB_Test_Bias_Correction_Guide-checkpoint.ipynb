{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3b7ff6",
   "metadata": {},
   "source": [
    "# A/B Test Bias Correction: A Practical Guide\n",
    "\n",
    "This notebook demonstrates the **common pitfalls and correct methods** for handling pre-existing bias (pre-bias) in A/B testing.\n",
    "\n",
    "We'll simulate data with and without pre-bias and compare four methods:\n",
    "- ‚ùå Naive Post-Only Comparison\n",
    "- ‚ùå Manual Bias Correction\n",
    "- ‚úÖ Difference-in-Differences (DiD)\n",
    "- ‚úÖ Regression Adjustment\n",
    "\n",
    "This is intended as a practical reference for analysts and data scientists who run controlled experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# Simulate pre-period KPI\n",
    "control_pre = np.random.normal(loc=100, scale=15, size=n)\n",
    "test_pre = np.random.normal(loc=105, scale=15, size=n)  # pre-bias +5\n",
    "\n",
    "# Simulate post-period KPI\n",
    "control_post = control_pre + np.random.normal(loc=2, scale=10, size=n)\n",
    "test_post = test_pre + np.random.normal(loc=5, scale=10, size=n)  # includes treatment effect +3\n",
    "\n",
    "# Combine data\n",
    "df = pd.DataFrame({\n",
    "    \"group\": [\"control\"] * n + [\"test\"] * n,\n",
    "    \"pre_kpi\": np.concatenate([control_pre, test_pre]),\n",
    "    \"post_kpi\": np.concatenate([control_post, test_post])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ebb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Post-Only Comparison\n",
    "mean_control_post = df[df.group == \"control\"][\"post_kpi\"].mean()\n",
    "mean_test_post = df[df.group == \"test\"][\"post_kpi\"].mean()\n",
    "naive_diff = mean_test_post - mean_control_post\n",
    "\n",
    "# Manual Bias Correction\n",
    "mean_control_pre = df[df.group == \"control\"][\"pre_kpi\"].mean()\n",
    "mean_test_pre = df[df.group == \"test\"][\"pre_kpi\"].mean()\n",
    "pre_bias = mean_test_pre - mean_control_pre\n",
    "manual_adjusted_diff = naive_diff - pre_bias\n",
    "\n",
    "# Difference-in-Differences (DiD)\n",
    "control_diff = control_post - control_pre\n",
    "test_diff = test_post - test_pre\n",
    "did_effect = test_diff.mean() - control_diff.mean()\n",
    "\n",
    "# Regression Adjustment\n",
    "df[\"group_binary\"] = df[\"group\"].apply(lambda x: 1 if x == \"test\" else 0)\n",
    "X = sm.add_constant(df[[\"group_binary\", \"pre_kpi\"]])\n",
    "model = sm.OLS(df[\"post_kpi\"], X).fit()\n",
    "regression_effect = model.params[\"group_binary\"]\n",
    "p_value = model.pvalues[\"group_binary\"]\n",
    "\n",
    "# Summary Table\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Scenario\": [\n",
    "        \"Naive (Post KPI Only)\",\n",
    "        \"Manual Bias Correction (Subtract Pre-Bias)\",\n",
    "        \"Difference-in-Differences (DiD)\",\n",
    "        \"Regression Adjustment\"\n",
    "    ],\n",
    "    \"Estimated Effect\": [\n",
    "        naive_diff,\n",
    "        manual_adjusted_diff,\n",
    "        did_effect,\n",
    "        regression_effect\n",
    "    ],\n",
    "    \"p-value\": [\n",
    "        \"‚ùå Not Reliable\", \n",
    "        \"‚ùå Not Reliable\", \n",
    "        \"‚úÖ Via DiD Test\", \n",
    "        f\"‚úÖ {p_value:.4f}\"\n",
    "    ]\n",
    "})\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Intervals\n",
    "se_control = np.std(control_diff, ddof=1) / np.sqrt(n)\n",
    "se_test = np.std(test_diff, ddof=1) / np.sqrt(n)\n",
    "se_did = np.sqrt(se_control**2 + se_test**2)\n",
    "ci_did_lower = did_effect - 1.96 * se_did\n",
    "ci_did_upper = did_effect + 1.96 * se_did\n",
    "\n",
    "conf_int_reg = model.conf_int().loc[\"group_binary\"]\n",
    "\n",
    "# Plot\n",
    "methods = [\"Naive\", \"Manual Correction\", \"DiD\", \"Regression\"]\n",
    "effects = [naive_diff, manual_adjusted_diff, did_effect, regression_effect]\n",
    "ci_lowers = [None, None, ci_did_lower, conf_int_reg[0]]\n",
    "ci_uppers = [None, None, ci_did_upper, conf_int_reg[1]]\n",
    "colors = [\"red\", \"orange\", \"green\", \"blue\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (method, effect, low, high, color) in enumerate(zip(methods, effects, ci_lowers, ci_uppers, colors)):\n",
    "    plt.scatter(i, effect, color=color, label=method, s=100)\n",
    "    if low is not None and high is not None:\n",
    "        plt.plot([i, i], [low, high], color=color, linestyle='-', linewidth=2)\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xticks(range(len(methods)), methods)\n",
    "plt.ylabel(\"Estimated Effect\")\n",
    "plt.title(\"Comparison of Bias Correction Methods with Confidence Intervals\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1cab5",
   "metadata": {},
   "source": [
    "## üß† Conclusion\n",
    "\n",
    "- **Naive post-only comparisons** can overstate the effect if there's any pre-bias.\n",
    "- **Manual bias correction** gives the correct point estimate but lacks statistical validity (no p-value or confidence interval).\n",
    "- **Difference-in-Differences (DiD)** and **Regression Adjustment** are statistically sound and recommended.\n",
    "- Always check for **pre-period imbalances** and use methods that account for them.\n",
    "\n",
    "You can reuse this notebook to validate the robustness of your A/B test results. Happy experimenting!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
